"""
H·ªá th·ªëng ƒë√°nh gi√° v√† so s√°nh 2 m√¥ h√¨nh decision:
1. Vector (ML) - s·ª≠ d·ª•ng decision tree v√† fuzzy logic v·ªõi vector inputs
2. Gemini (LLM) - s·ª≠ d·ª•ng Gemini API ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh
"""

from __future__ import annotations

import os
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)
import json
import time
from datetime import datetime

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenv not installed, try manual loading
    try:
        if os.path.exists('.env'):
            with open('.env', 'r', encoding='utf-8-sig') as f:  # utf-8-sig removes BOM
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
    except Exception:
        pass
except Exception:
    # If load_dotenv fails, try manual loading
    try:
        if os.path.exists('.env'):
            with open('.env', 'r', encoding='utf-8-sig') as f:  # utf-8-sig removes BOM
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
    except Exception:
        pass

from main import MASClinicalDecisionSystem
from agents.decision_agent import DecisionAgent, LLMDecisionMaker


class DecisionModelEvaluator:
    """ƒê√°nh gi√° v√† so s√°nh 2 m√¥ h√¨nh decision: Vector (ML) v√† Gemini (LLM)"""
    
    def __init__(
        self,
        system: MASClinicalDecisionSystem,
        gemini_api_key: Optional[str] = None,
        enable_vector: bool = True,
        enable_llm: bool = True
    ):
        """
        Args:
            system: MASClinicalDecisionSystem ƒë√£ ƒë∆∞·ª£c train
            gemini_api_key: API key cho Gemini (n·∫øu d√πng LLM)
            enable_vector: Enable Vector (ML) model
            enable_llm: Enable Gemini (LLM) model
        """
        self.system = system
        self.enable_vector = enable_vector
        self.enable_llm = enable_llm
        
        # T·∫°o decision agents d·ª±a tr√™n flags
        self.vector_agent = None
        self.gemini_agent = None
        
        if enable_vector:
            # Agent 1: Ch·ªâ d√πng Vector (ML) - kh√¥ng d√πng LLM
            self.vector_agent = DecisionAgent(
                use_fuzzy=True,
                use_decision_tree=True,
                use_llm=False,
                decision_mode="vector_only"  # Ch·ªâ d√πng vector
            )
            print(f"  ‚úì Vector (ML) model ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t")
        
        if enable_llm:
            # Agent 2: Ch·ªâ d√πng Gemini (LLM)
            self.gemini_agent = DecisionAgent(
                use_fuzzy=True,
                use_decision_tree=True,
                use_llm=True,
                llm_api_key=gemini_api_key,
                decision_mode="llm_only"  # Ch·ªâ d√πng LLM
            )
        
            # Ki·ªÉm tra xem LLM c√≥ available kh√¥ng
            if self.gemini_agent.llm_decision_maker:
                if self.gemini_agent.llm_decision_maker.available:
                    print(f"  ‚úì Gemini LLM ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t")
                    # Test LLM v·ªõi m·ªôt sample ƒë∆°n gi·∫£n
                    try:
                        test_result = self.gemini_agent.llm_decision_maker.generate_decision(
                            patient_data={"age": 50, "gender": "M"},
                            probabilities={"AMX/AMP": 0.8, "CIP": 0.6},
                            critic_report={"decision": {"uncertainty_score": 0.2}},
                            explanation={"decision": {"decision_score": 0.7}}
                        )
                        if test_result.get("decision_type") not in ["llm_unavailable", "llm_error"]:
                            print(f"  ‚úì Test LLM th√†nh c√¥ng: {test_result.get('decision_type')}")
                        else:
                            print(f"  ‚ö†Ô∏è  Test LLM th·∫•t b·∫°i: {test_result.get('decision_type')}")
                            if test_result.get("error"):
                                print(f"     Error: {test_result.get('error')}")
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è  Test LLM exception: {str(e)}")
                else:
                    print(f"  ‚ö†Ô∏è  Gemini LLM kh√¥ng available (ki·ªÉm tra API key)")
            else:
                print(f"  ‚ö†Ô∏è  LLMDecisionMaker ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        
        if not enable_vector and not enable_llm:
            raise ValueError("Ph·∫£i enable √≠t nh·∫•t 1 m√¥ h√¨nh (enable_vector ho·∫∑c enable_llm)")
    
    def _normalize_decision_type(self, decision_type: str) -> str:
        """Chu·∫©n h√≥a decision_type v·ªÅ 3 lo·∫°i: treat, review, test"""
        decision_lower = decision_type.lower()
        
        if "treat" in decision_lower or "high_confidence" in decision_lower:
            return "treat"
        elif "test" in decision_lower or "additional" in decision_lower or "requires" in decision_lower:
            return "test"
        else:
            return "review"
    
    def _get_ground_truth_decision(
        self,
        probabilities: pd.DataFrame,
        patient_features: Optional[pd.Series],
        critic_report: Dict
    ) -> str:
        """
        T·∫°o ground truth decision d·ª±a tr√™n heuristic c·∫£i ti·∫øn:
        - treat: n·∫øu c√≥ antibiotic v·ªõi prob r·∫•t cao (>= 0.8) ho·∫∑c prob cao (>= 0.7) v·ªõi uncertainty th·∫•p
        - test: n·∫øu kh√¥ng c√≥ antibiotic n√†o t·ªët ho·∫∑c uncertainty r·∫•t cao
        - review: c√°c tr∆∞·ªùng h·ª£p trung gian
        """
        if probabilities.empty:
            return "test"
        
        proba_series = probabilities.iloc[0]
        max_prob = proba_series.max()
        mean_prob = proba_series.mean()
        
        # ƒê·∫øm s·ªë antibiotic c√≥ prob >= 0.5, >= 0.7, v√† >= 0.8
        very_high_prob_count = (proba_series >= 0.8).sum()
        high_prob_count = (proba_series >= 0.7).sum()
        medium_prob_count = (proba_series >= 0.5).sum()
        
        uncertainty = critic_report.get("decision", {}).get("uncertainty_score", 0.5)
        risk_factors = patient_features.get("Total_risk_factors", 0) if patient_features is not None else 0
        
        # Heuristic rules c·∫£i ti·∫øn - ∆∞u ti√™n max_prob cao
        # TREAT: 
        # 1. C√≥ √≠t nh·∫•t 1 antibiotic v·ªõi prob r·∫•t cao (>= 0.8) v√† uncertainty kh√¥ng qu√° cao
        if max_prob >= 0.8 and uncertainty < 0.6 and risk_factors <= 3:
            return "treat"
        # 2. C√≥ √≠t nh·∫•t 1 antibiotic v·ªõi prob cao (>= 0.75) v√† uncertainty th·∫•p
        elif max_prob >= 0.75 and uncertainty < 0.35 and risk_factors <= 2:
            return "treat"
        # 3. C√≥ nhi·ªÅu antibiotic v·ªõi prob t·ªët (>= 0.7) v√† uncertainty th·∫•p
        elif high_prob_count >= 2 and uncertainty < 0.4 and risk_factors <= 2:
            return "treat"
        
        # TEST: 
        # 1. Kh√¥ng c√≥ antibiotic n√†o t·ªët
        elif max_prob < 0.4 or medium_prob_count == 0:
            return "test"
        # 2. Uncertainty r·∫•t cao
        elif uncertainty > 0.75:
            return "test"
        # 3. Risk r·∫•t cao
        elif risk_factors >= 5:
            return "test"
        
        # REVIEW: C√°c tr∆∞·ªùng h·ª£p trung gian
        # 1. C√≥ antibiotic v·ªõi prob t·ªët nh∆∞ng uncertainty trung b√¨nh-cao
        elif max_prob >= 0.7 and uncertainty >= 0.4 and uncertainty < 0.7:
            return "review"
        # 2. C√≥ antibiotic v·ªõi prob trung b√¨nh (0.5-0.7)
        elif max_prob >= 0.5 and max_prob < 0.7:
            return "review"
        # 3. C√≥ √≠t antibiotic v·ªõi prob t·ªët nh∆∞ng uncertainty trung b√¨nh
        elif medium_prob_count >= 1 and medium_prob_count < 3 and uncertainty < 0.6:
            return "review"
        # M·∫∑c ƒë·ªãnh: review cho c√°c tr∆∞·ªùng h·ª£p kh√¥ng r√µ r√†ng
        else:
            return "review"
    
    def evaluate_on_dataset(
        self,
        csv_path: str,
        n_samples: Optional[int] = None,
        use_ground_truth: bool = True,
        llm_delay: float = 0.0
    ) -> Dict:
        """
        ƒê√°nh gi√° 2 m√¥ h√¨nh tr√™n dataset.
        
        Args:
            csv_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV
            n_samples: S·ªë l∆∞·ª£ng m·∫´u ƒë·ªÉ ƒë√°nh gi√° (None = t·∫•t c·∫£)
            use_ground_truth: N·∫øu True, s·ª≠ d·ª•ng ground truth heuristic. N·∫øu False, ch·ªâ so s√°nh 2 m√¥ h√¨nh v·ªõi nhau.
            llm_delay: Th·ªùi gian delay (gi√¢y) gi·ªØa m·ªói l·∫ßn g·ªçi LLM ƒë·ªÉ tr√°nh quota. M·∫∑c ƒë·ªãnh 0.0 (kh√¥ng delay).
        """
        print("=" * 80)
        print("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å 2 M√î H√åNH DECISION")
        print("=" * 80)
        
        # ƒê·ªçc d·ªØ li·ªáu
        df = pd.read_csv(csv_path)
        if n_samples:
            df = df.head(n_samples)
        
        print(f"  ‚úì ƒê√£ t·∫£i {len(df)} m·∫´u t·ª´ dataset")
        
        # Th√¥ng b√°o v·ªÅ delay n·∫øu c√≥
        if llm_delay > 0 and self.enable_llm:
            total_delay_time = llm_delay * (len(df) - 1) if len(df) > 1 else 0
            print(f"  ‚è≥ LLM delay: {llm_delay}s gi·ªØa m·ªói m·∫´u (t·ªïng th·ªùi gian delay: ~{total_delay_time:.0f}s)")
        
        # K·∫øt qu·∫£ - ch·ªâ kh·ªüi t·∫°o cho c√°c m√¥ h√¨nh ƒë∆∞·ª£c enable
        vector_predictions = []
        gemini_predictions = []
        ground_truths = []
        vector_scores = []
        gemini_scores = []
        vector_methods = []
        gemini_methods = []
        
        # Ki·ªÉm tra c√≥ √≠t nh·∫•t 1 m√¥ h√¨nh ƒë∆∞·ª£c enable
        if not self.enable_vector and not self.enable_llm:
            raise ValueError("Ph·∫£i enable √≠t nh·∫•t 1 m√¥ h√¨nh ƒë·ªÉ ƒë√°nh gi√°")
        
        print("\n  ƒêang ch·∫°y ƒë√°nh gi√° tr√™n t·ª´ng m·∫´u...")
        
        for idx, row in df.iterrows():
            if (idx + 1) % 10 == 0:
                print(f"    ƒê√£ x·ª≠ l√Ω {idx + 1}/{len(df)} m·∫´u...")
            
            try:
                # Chu·∫©n b·ªã d·ªØ li·ªáu b·ªánh nh√¢n
                patient = {
                    "age/gender": row.get("age/gender", ""),
                    "Souches": row.get("Souches", ""),
                    "Diabetes": "Yes" if row.get("Diabetes") in ["Yes", True, 1] else "No",
                    "Hypertension": "Yes" if row.get("Hypertension") in ["Yes", True, 1] else "No",
                    "Hospital_before": "Yes" if row.get("Hospital_before") in ["Yes", True, 1] else "No",
                    "Infection_Freq": row.get("Infection_Freq", 0.0),
                    "Collection_Date": row.get("Collection_Date", ""),
                }
                
                # Ch·∫°y pipeline ƒë·ªÉ l·∫•y c√°c inputs c·∫ßn thi·∫øt
                result = self.system.predict(patient)
                
                # Chuy·ªÉn ƒë·ªïi dict th√†nh DataFrame/Series ƒë√∫ng c√°ch
                # probabilities v√† predictions t·ª´ result l√† dict, c·∫ßn chuy·ªÉn th√†nh DataFrame
                probabilities_dict = result["probabilities"]
                predictions_dict = result["predictions"]
                
                # T·∫°o DataFrame v·ªõi index [0] ƒë·ªÉ c√≥ 1 row
                # ƒê·∫£m b·∫£o DataFrame c√≥ ƒë√∫ng c·∫•u tr√∫c
                probabilities = pd.DataFrame([probabilities_dict], index=[0])
                predictions = pd.DataFrame([predictions_dict], index=[0])
                patient_series = pd.Series(result["features"])
                
                # Ki·ªÉm tra DataFrame kh√¥ng r·ªóng v√† c√≥ d·ªØ li·ªáu h·ª£p l·ªá
                if probabilities.empty or len(probabilities) == 0:
                    print(f"    ‚ö†Ô∏è  Probabilities r·ªóng cho m·∫´u {idx}")
                    continue
                
                # ƒê·∫£m b·∫£o predictions v√† probabilities c√≥ c√πng s·ªë c·ªôt
                if len(predictions.columns) == 0 or len(probabilities.columns) == 0:
                    print(f"    ‚ö†Ô∏è  D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá cho m·∫´u {idx}")
                    continue
                
                critic_report = result["critic_report"]
                explanation = result["explanation"]
                
                # L·∫•y vectors t·ª´ Agent 3 v√† Agent 4
                # explain_vector v√† review_vector c·∫ßn DataFrame, kh√¥ng ph·∫£i Series
                try:
                    explain_vector = self.system.pipeline.explain_agent.explain_vector(
                        patient_series,
                        predictions,  # DataFrame, kh√¥ng ph·∫£i Series
                        probabilities  # DataFrame, kh√¥ng ph·∫£i Series
                    )
                    critic_vector = self.system.pipeline.critic_agent.review_vector(
                        probabilities,
                        patient_series
                    )
                except Exception as vec_error:
                    print(f"    ‚ö†Ô∏è  L·ªói khi t·∫°o vectors cho m·∫´u {idx}: {str(vec_error)}")
                    import traceback
                    traceback.print_exc()
                    continue
                
                # 1. Ch·∫°y Vector (ML) model (n·∫øu ƒë∆∞·ª£c enable)
                if self.enable_vector and self.vector_agent:
                    vector_decision = self.vector_agent.decide(
                        probabilities,
                        critic_report,
                        patient_series,
                        explanation=explanation,
                        explain_vector=explain_vector,
                        critic_vector=critic_vector
                    )
                    
                    vector_decision_type = self._normalize_decision_type(
                        vector_decision["decision"].get("decision_type", "review")
                    )
                    vector_predictions.append(vector_decision_type)
                    vector_scores.append(vector_decision["decision"].get("decision_score", 0.5))
                    vector_methods.append(vector_decision["decision"].get("method", "unknown"))
                
                # 2. Ch·∫°y Gemini (LLM) model (n·∫øu ƒë∆∞·ª£c enable)
                if self.enable_llm and self.gemini_agent:
                    # Kh√¥ng truy·ªÅn vectors cho LLM-only mode ƒë·ªÉ force s·ª≠ d·ª•ng LLM
                    gemini_decision = self.gemini_agent.decide(
                        probabilities,
                        critic_report,
                        patient_series,
                        explanation=explanation,
                        explain_vector=None,  # Kh√¥ng truy·ªÅn vectors ƒë·ªÉ force LLM
                        critic_vector=None
                    )
                    
                    raw_decision_type = gemini_decision["decision"].get("decision_type", "review")
                    gemini_decision_type = self._normalize_decision_type(raw_decision_type)
                    gemini_predictions.append(gemini_decision_type)
                    gemini_scores.append(gemini_decision["decision"].get("decision_score", 0.5))
                    gemini_method = gemini_decision["decision"].get("method", "unknown")
                    gemini_methods.append(gemini_method)
                    
                    # Debug: Log raw decision type v√† normalized cho v√†i m·∫´u ƒë·∫ßu
                    if idx < 5:
                        print(f"    DEBUG Sample {idx+1}: raw_decision='{raw_decision_type}' -> normalized='{gemini_decision_type}', method='{gemini_method}'")
                    
                    # Debug: Log n·∫øu kh√¥ng d√πng LLM (ch·ªâ log 1 l·∫ßn ƒë·∫ßu ti√™n)
                    if idx == 0 and "llm" not in gemini_method.lower() and "gemini" not in gemini_method.lower():
                        print(f"\n    ‚ö†Ô∏è  DEBUG: Gemini model ƒëang d√πng method '{gemini_method}' thay v√¨ LLM")
                        if gemini_decision["decision"].get("error"):
                            print(f"    ‚ö†Ô∏è  LLM Error: {gemini_decision['decision'].get('error')}")
                    
                    # Delay gi·ªØa c√°c l·∫ßn g·ªçi LLM ƒë·ªÉ tr√°nh quota (ch·ªâ delay n·∫øu d√πng LLM v√† kh√¥ng ph·∫£i m·∫´u cu·ªëi)
                    if (llm_delay > 0 and 
                        self.enable_llm and 
                        self.gemini_agent and 
                        self.gemini_agent.llm_decision_maker and
                        self.gemini_agent.llm_decision_maker.available and
                        "llm" in gemini_method.lower() and 
                        idx < len(df) - 1):
                        print(f"    ‚è≥ ƒêang delay {llm_delay}s tr∆∞·ªõc m·∫´u ti·∫øp theo...")
                        time.sleep(llm_delay)
                
                # 3. Ground truth (n·∫øu c·∫ßn) - t√≠nh cho m·ªói m·∫´u ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng
                # Ground truth ph·∫£i ƒë∆∞·ª£c t√≠nh cho m·ªói m·∫´u, kh√¥ng ph·ª• thu·ªôc v√†o s·ªë l∆∞·ª£ng predictions
                if use_ground_truth:
                    # T√≠nh ground truth d·ª±a tr√™n s·ªë m·∫´u ƒë√£ x·ª≠ l√Ω (t·ªëi ƒëa l√† s·ªë predictions ƒë√£ c√≥)
                    # ƒê·∫£m b·∫£o m·ªói m·∫´u ch·ªâ c√≥ 1 ground truth
                    current_sample_idx = max(len(vector_predictions), len(gemini_predictions), len(ground_truths))
                    if len(ground_truths) < current_sample_idx:
                        gt = self._get_ground_truth_decision(
                            probabilities,
                            patient_series,
                            critic_report
                        )
                        ground_truths.append(gt)
                        
                        # Debug: Log th√¥ng tin cho v√†i m·∫´u ƒë·∫ßu ti√™n
                        if len(ground_truths) <= 5:
                            proba_series = probabilities.iloc[0]
                            max_prob = proba_series.max()
                            uncertainty = critic_report.get("decision", {}).get("uncertainty_score", 0.5)
                            risk_factors = patient_series.get("Total_risk_factors", 0) if patient_series is not None else 0
                            print(f"    DEBUG Sample {len(ground_truths)}: max_prob={max_prob:.3f}, uncertainty={uncertainty:.3f}, risk={risk_factors}, GT={gt}")
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è  L·ªói khi x·ª≠ l√Ω m·∫´u {idx}: {str(e)}")
                continue
        
        # T√≠nh s·ªë m·∫´u ƒë√£ x·ª≠ l√Ω v√† ƒë·∫£m b·∫£o ƒë·ªô d√†i kh·ªõp nhau
        n_processed = max(len(vector_predictions), len(gemini_predictions))
        
        # ƒê·∫£m b·∫£o ground_truths c√≥ c√πng ƒë·ªô d√†i v·ªõi s·ªë m·∫´u ƒë√£ x·ª≠ l√Ω
        if use_ground_truth:
            while len(ground_truths) < n_processed:
                # N·∫øu thi·∫øu ground truth, th√™m gi√° tr·ªã m·∫∑c ƒë·ªãnh (kh√¥ng n√™n x·∫£y ra)
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: Thi·∫øu ground truth cho {n_processed - len(ground_truths)} m·∫´u")
                break
            # C·∫Øt b·ªõt n·∫øu th·ª´a
            if len(ground_truths) > n_processed:
                ground_truths = ground_truths[:n_processed]
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: ƒê√£ c·∫Øt b·ªõt ground_truths t·ª´ {len(ground_truths)} xu·ªëng {n_processed}")
        
        # ƒê·∫£m b·∫£o vector_predictions v√† gemini_predictions c√≥ c√πng ƒë·ªô d√†i
        if self.enable_vector and self.enable_llm:
            min_len = min(len(vector_predictions), len(gemini_predictions))
            if len(vector_predictions) != len(gemini_predictions):
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: Vector c√≥ {len(vector_predictions)} predictions, Gemini c√≥ {len(gemini_predictions)} predictions")
                # C·∫Øt v·ªÅ ƒë·ªô d√†i nh·ªè nh·∫•t ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh metrics ch√≠nh x√°c
                vector_predictions = vector_predictions[:min_len]
                gemini_predictions = gemini_predictions[:min_len]
                if use_ground_truth:
                    ground_truths = ground_truths[:min_len]
                n_processed = min_len
        
        print(f"\n  ‚úì ƒê√£ ho√†n th√†nh ƒë√°nh gi√° {n_processed} m·∫´u")
        if use_ground_truth:
            print(f"  ‚úì S·ªë l∆∞·ª£ng ground_truths: {len(ground_truths)}")
        if self.enable_vector:
            print(f"  ‚úì S·ªë l∆∞·ª£ng vector_predictions: {len(vector_predictions)}")
        if self.enable_llm:
            print(f"  ‚úì S·ªë l∆∞·ª£ng gemini_predictions: {len(gemini_predictions)}")
        
        # Debug: Ki·ªÉm tra ph√¢n b·ªë predictions
        if self.enable_llm and len(gemini_predictions) > 0:
            from collections import Counter
            gemini_dist = Counter(gemini_predictions)
            print(f"\n  üìä Ph√¢n b·ªë Gemini predictions: {dict(gemini_dist)}")
            if len(gemini_dist) == 1:
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: T·∫•t c·∫£ Gemini predictions ƒë·ªÅu gi·ªëng nhau: {list(gemini_dist.keys())[0]}")
        
        if use_ground_truth and len(ground_truths) > 0:
            from collections import Counter
            gt_dist = Counter(ground_truths)
            print(f"  üìä Ph√¢n b·ªë Ground Truth: {dict(gt_dist)}")
            if len(gt_dist) == 1:
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: T·∫•t c·∫£ Ground Truth ƒë·ªÅu gi·ªëng nhau: {list(gt_dist.keys())[0]}")
            
            # Ph√¢n t√≠ch chi ti·∫øt h∆°n
            if self.enable_vector and len(vector_predictions) > 0:
                vector_dist = Counter(vector_predictions)
                print(f"  üìä Ph√¢n b·ªë Vector predictions: {dict(vector_dist)}")
                # So s√°nh v·ªõi ground truth
                matches = sum(1 for gt, pred in zip(ground_truths, vector_predictions) if gt == pred)
                print(f"  üìä Vector matches GT: {matches}/{len(ground_truths)} ({matches/len(ground_truths)*100:.1f}%)")
            
            if self.enable_llm and len(gemini_predictions) > 0:
                gemini_dist = Counter(gemini_predictions)
                print(f"  üìä Ph√¢n b·ªë Gemini predictions: {dict(gemini_dist)}")
                # So s√°nh v·ªõi ground truth
                matches = sum(1 for gt, pred in zip(ground_truths, gemini_predictions) if gt == pred)
                print(f"  üìä Gemini matches GT: {matches}/{len(ground_truths)} ({matches/len(ground_truths)*100:.1f}%)")
        
        # T√≠nh metrics
        results = {
            "n_samples": n_processed,
            "enable_vector": self.enable_vector,
            "enable_llm": self.enable_llm,
            "vector_predictions": vector_predictions if self.enable_vector else [],
            "gemini_predictions": gemini_predictions if self.enable_llm else [],
            "vector_scores": vector_scores if self.enable_vector else [],
            "gemini_scores": gemini_scores if self.enable_llm else [],
            "vector_methods": vector_methods if self.enable_vector else [],
            "gemini_methods": gemini_methods if self.enable_llm else [],
        }
        
        if use_ground_truth:
            results["ground_truths"] = ground_truths
            results["metrics"] = self._calculate_metrics_with_ground_truth(
                ground_truths,
                vector_predictions if self.enable_vector else [],
                gemini_predictions if self.enable_llm else []
            )
        else:
            results["metrics"] = self._calculate_metrics_comparison(
                vector_predictions if self.enable_vector else [],
                gemini_predictions if self.enable_llm else [],
                vector_scores if self.enable_vector else [],
                gemini_scores if self.enable_llm else []
            )
        
        return results
    
    def _calculate_metrics_with_ground_truth(
        self,
        ground_truths: List[str],
        vector_predictions: List[str],
        gemini_predictions: List[str]
    ) -> Dict:
        """T√≠nh metrics khi c√≥ ground truth"""
        labels = ["treat", "review", "test"]
        metrics = {}
        
        # Validation: ƒê·∫£m b·∫£o ƒë·ªô d√†i kh·ªõp nhau
        if self.enable_vector and len(vector_predictions) > 0:
            if len(ground_truths) != len(vector_predictions):
                print(f"  ‚ö†Ô∏è  L·ªñI: ground_truths ({len(ground_truths)}) v√† vector_predictions ({len(vector_predictions)}) c√≥ ƒë·ªô d√†i kh√°c nhau!")
                min_len = min(len(ground_truths), len(vector_predictions))
                ground_truths = ground_truths[:min_len]
                vector_predictions = vector_predictions[:min_len]
                print(f"  ‚ö†Ô∏è  ƒê√£ c·∫Øt v·ªÅ ƒë·ªô d√†i {min_len} ƒë·ªÉ t√≠nh metrics")
        
        if self.enable_llm and len(gemini_predictions) > 0:
            if len(ground_truths) != len(gemini_predictions):
                print(f"  ‚ö†Ô∏è  L·ªñI: ground_truths ({len(ground_truths)}) v√† gemini_predictions ({len(gemini_predictions)}) c√≥ ƒë·ªô d√†i kh√°c nhau!")
                min_len = min(len(ground_truths), len(gemini_predictions))
                ground_truths = ground_truths[:min_len]
                gemini_predictions = gemini_predictions[:min_len]
                print(f"  ‚ö†Ô∏è  ƒê√£ c·∫Øt v·ªÅ ƒë·ªô d√†i {min_len} ƒë·ªÉ t√≠nh metrics")
        
        # Metrics cho Vector model (n·∫øu ƒë∆∞·ª£c enable)
        if self.enable_vector and len(vector_predictions) > 0:
            # Debug: In ra m·ªôt v√†i v√≠ d·ª• ƒë·ªÉ ki·ªÉm tra
            if len(vector_predictions) <= 10:
                print(f"\n  üîç DEBUG Vector Model:")
                print(f"    Ground Truth: {ground_truths}")
                print(f"    Predictions:  {vector_predictions}")
                matches = sum(1 for gt, pred in zip(ground_truths, vector_predictions) if gt == pred)
                print(f"    Matches: {matches}/{len(ground_truths)}")
            
            vector_accuracy = accuracy_score(ground_truths, vector_predictions)
            vector_precision = precision_score(ground_truths, vector_predictions, labels=labels, average="weighted", zero_division=0)
            vector_recall = recall_score(ground_truths, vector_predictions, labels=labels, average="weighted", zero_division=0)
            vector_f1 = f1_score(ground_truths, vector_predictions, labels=labels, average="weighted", zero_division=0)
            vector_cm = confusion_matrix(ground_truths, vector_predictions, labels=labels)
            vector_report = classification_report(ground_truths, vector_predictions, labels=labels, output_dict=True, zero_division=0)
            
            metrics["vector_model"] = {
                "accuracy": float(vector_accuracy),
                "precision": float(vector_precision),
                "recall": float(vector_recall),
                "f1_score": float(vector_f1),
                "confusion_matrix": vector_cm.tolist(),
                "classification_report": vector_report
            }
        
        # Metrics cho Gemini model (n·∫øu ƒë∆∞·ª£c enable)
        if self.enable_llm and len(gemini_predictions) > 0:
            # Debug: In ra m·ªôt v√†i v√≠ d·ª• ƒë·ªÉ ki·ªÉm tra
            if len(gemini_predictions) <= 10:
                print(f"\n  üîç DEBUG Gemini Model:")
                print(f"    Ground Truth: {ground_truths}")
                print(f"    Predictions:  {gemini_predictions}")
                matches = sum(1 for gt, pred in zip(ground_truths, gemini_predictions) if gt == pred)
                print(f"    Matches: {matches}/{len(ground_truths)}")
                # In chi ti·∫øt t·ª´ng c·∫∑p ƒë·ªÉ debug
                print(f"    Chi ti·∫øt so s√°nh:")
                for i, (gt, pred) in enumerate(zip(ground_truths, gemini_predictions)):
                    match_symbol = "‚úì" if gt == pred else "‚úó"
                    print(f"      Sample {i+1}: GT={gt:6} vs Pred={pred:6} {match_symbol}")
            
            # Ki·ªÉm tra ph√¢n b·ªë classes
            from collections import Counter
            gt_dist = Counter(ground_truths)
            pred_dist = Counter(gemini_predictions)
            
            # C·∫£nh b√°o n·∫øu ch·ªâ c√≥ m·ªôt class duy nh·∫•t
            if len(gt_dist) == 1:
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: Ground truth ch·ªâ c√≥ 1 class duy nh·∫•t: {list(gt_dist.keys())[0]}")
            if len(pred_dist) == 1:
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: Gemini predictions ch·ªâ c√≥ 1 class duy nh·∫•t: {list(pred_dist.keys())[0]}")
            
            gemini_accuracy = accuracy_score(ground_truths, gemini_predictions)
            gemini_precision = precision_score(ground_truths, gemini_predictions, labels=labels, average="weighted", zero_division=0)
            gemini_recall = recall_score(ground_truths, gemini_predictions, labels=labels, average="weighted", zero_division=0)
            gemini_f1 = f1_score(ground_truths, gemini_predictions, labels=labels, average="weighted", zero_division=0)
            gemini_cm = confusion_matrix(ground_truths, gemini_predictions, labels=labels)
            gemini_report = classification_report(ground_truths, gemini_predictions, labels=labels, output_dict=True, zero_division=0)
            
            # C·∫£nh b√°o n·∫øu t·∫•t c·∫£ metrics ƒë·ªÅu b·∫±ng 1.0
            if gemini_accuracy == 1.0 and gemini_precision == 1.0 and gemini_recall == 1.0 and gemini_f1 == 1.0:
                print(f"  ‚ÑπÔ∏è  TH√îNG TIN: T·∫•t c·∫£ metrics ƒë·ªÅu b·∫±ng 1.0 - ƒê√¢y l√† k·∫øt qu·∫£ ƒê√öNG khi:")
                print(f"      ‚úì T·∫•t c·∫£ predictions ƒë·ªÅu kh·ªõp v·ªõi ground truth (100% accuracy)")
                print(f"      ‚úì Kh√¥ng c√≥ false positives (100% precision)")
                print(f"      ‚úì Kh√¥ng c√≥ false negatives (100% recall)")
                print(f"      - Ph√¢n b·ªë GT: {dict(gt_dist)}")
                print(f"      - Ph√¢n b·ªë Predictions: {dict(pred_dist)}")
                if len(gemini_predictions) < 20:
                    print(f"  ‚ö†Ô∏è  L∆ØU √ù: V·ªõi ch·ªâ {len(gemini_predictions)} m·∫´u, k·∫øt qu·∫£ c√≥ th·ªÉ kh√¥ng ƒë·∫°i di·ªán.")
                    print(f"      Khuy·∫øn ngh·ªã: TƒÉng s·ªë l∆∞·ª£ng m·∫´u l√™n √≠t nh·∫•t 20-50 ƒë·ªÉ ƒë√°nh gi√° ƒë√°ng tin c·∫≠y h∆°n.")
            
            metrics["gemini_model"] = {
                "accuracy": float(gemini_accuracy),
                "precision": float(gemini_precision),
                "recall": float(gemini_recall),
                "f1_score": float(gemini_f1),
                "confusion_matrix": gemini_cm.tolist(),
                "classification_report": gemini_report
            }
        
        # So s√°nh (ch·ªâ khi c·∫£ 2 ƒë·ªÅu ƒë∆∞·ª£c enable)
        if self.enable_vector and self.enable_llm and len(vector_predictions) > 0 and len(gemini_predictions) > 0:
            vector_accuracy = metrics["vector_model"]["accuracy"]
            gemini_accuracy = metrics["gemini_model"]["accuracy"]
            vector_f1 = metrics["vector_model"]["f1_score"]
            gemini_f1 = metrics["gemini_model"]["f1_score"]
            vector_precision = metrics["vector_model"]["precision"]
            gemini_precision = metrics["gemini_model"]["precision"]
            vector_recall = metrics["vector_model"]["recall"]
            gemini_recall = metrics["gemini_model"]["recall"]
            
            metrics["comparison"] = {
                "accuracy_diff": float(gemini_accuracy - vector_accuracy),
                "precision_diff": float(gemini_precision - vector_precision),
                "recall_diff": float(gemini_recall - vector_recall),
                "f1_diff": float(gemini_f1 - vector_f1),
                "winner_accuracy": "gemini" if gemini_accuracy > vector_accuracy else "vector",
                "winner_f1": "gemini" if gemini_f1 > vector_f1 else "vector"
            }
        
        return metrics
    
    def _calculate_metrics_comparison(
        self,
        vector_predictions: List[str],
        gemini_predictions: List[str],
        vector_scores: List[float],
        gemini_scores: List[float]
    ) -> Dict:
        """T√≠nh metrics khi so s√°nh tr·ª±c ti·∫øp 2 m√¥ h√¨nh (kh√¥ng c√≥ ground truth)"""
        # Agreement rate
        agreement = sum(1 for v, g in zip(vector_predictions, gemini_predictions) if v == g)
        agreement_rate = agreement / len(vector_predictions) if vector_predictions else 0
        
        # Score statistics
        vector_mean_score = np.mean(vector_scores) if vector_scores else 0
        gemini_mean_score = np.mean(gemini_scores) if gemini_scores else 0
        vector_std_score = np.std(vector_scores) if vector_scores else 0
        gemini_std_score = np.std(gemini_scores) if gemini_scores else 0
        
        # Distribution of decisions
        from collections import Counter
        vector_dist = Counter(vector_predictions)
        gemini_dist = Counter(gemini_predictions)
        
        return {
            "agreement_rate": float(agreement_rate),
            "vector_model": {
                "mean_score": float(vector_mean_score),
                "std_score": float(vector_std_score),
                "decision_distribution": dict(vector_dist)
            },
            "gemini_model": {
                "mean_score": float(gemini_mean_score),
                "std_score": float(gemini_std_score),
                "decision_distribution": dict(gemini_dist)
            },
            "comparison": {
                "score_diff": float(gemini_mean_score - vector_mean_score),
                "agreement_rate": float(agreement_rate)
            }
        }
    
    def print_results(self, results: Dict):
        """In k·∫øt qu·∫£ ƒë√°nh gi√°"""
        print("\n" + "=" * 80)
        print("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH DECISION")
        print("=" * 80)
        
        print(f"\nüìä S·ªë l∆∞·ª£ng m·∫´u ƒë√°nh gi√°: {results['n_samples']}")
        print(f"üìä M√¥ h√¨nh ƒë∆∞·ª£c ƒë√°nh gi√°:")
        if results.get('enable_vector'):
            print(f"  ‚úì Vector (ML) model")
        if results.get('enable_llm'):
            print(f"  ‚úì Gemini (LLM) model")
        
        if "ground_truths" in results:
            # C√≥ ground truth
            metrics = results["metrics"]
            
            print("\n" + "-" * 80)
            print("METRICS V·ªöI GROUND TRUTH")
            print("-" * 80)
            
            if results.get('enable_vector') and 'vector_model' in metrics:
                print("\nüîµ VECTOR (ML) MODEL:")
                print(f"  Accuracy:  {metrics['vector_model']['accuracy']:.4f}")
                print(f"  Precision: {metrics['vector_model']['precision']:.4f}")
                print(f"  Recall:    {metrics['vector_model']['recall']:.4f}")
                print(f"  F1-Score:  {metrics['vector_model']['f1_score']:.4f}")
                
                # Hi·ªÉn th·ªã per-class metrics
                if 'classification_report' in metrics['vector_model']:
                    report = metrics['vector_model']['classification_report']
                    print("\n  Per-class metrics:")
                    labels = ["treat", "review", "test"]
                    for label in labels:
                        if label in report:
                            prec = report[label].get('precision', 0)
                            rec = report[label].get('recall', 0)
                            f1 = report[label].get('f1-score', 0)
                            support = report[label].get('support', 0)
                            print(f"    {label:6}: Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}, Support={support}")
                
                print("\nüìä CONFUSION MATRIX - VECTOR MODEL:")
                print("        treat  review  test")
                cm = metrics['vector_model']['confusion_matrix']
                labels = ["treat", "review", "test"]
                for i, label in enumerate(labels):
                    print(f"{label:8} {cm[i]}")
            
            if results.get('enable_llm') and 'gemini_model' in metrics:
                print("\nüü¢ GEMINI (LLM) MODEL:")
                print(f"  Accuracy:  {metrics['gemini_model']['accuracy']:.4f}")
                print(f"  Precision: {metrics['gemini_model']['precision']:.4f}")
                print(f"  Recall:    {metrics['gemini_model']['recall']:.4f}")
                print(f"  F1-Score:  {metrics['gemini_model']['f1_score']:.4f}")
                
                # Hi·ªÉn th·ªã per-class metrics
                if 'classification_report' in metrics['gemini_model']:
                    report = metrics['gemini_model']['classification_report']
                    print("\n  Per-class metrics:")
                    labels = ["treat", "review", "test"]
                    for label in labels:
                        if label in report:
                            prec = report[label].get('precision', 0)
                            rec = report[label].get('recall', 0)
                            f1 = report[label].get('f1-score', 0)
                            support = report[label].get('support', 0)
                            print(f"    {label:6}: Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}, Support={support}")
                
                print("\nüìä CONFUSION MATRIX - GEMINI MODEL:")
                print("        treat  review  test")
                cm = metrics['gemini_model']['confusion_matrix']
                labels = ["treat", "review", "test"]
                for i, label in enumerate(labels):
                    print(f"{label:8} {cm[i]}")
            
            if 'comparison' in metrics:
                print("\nüìà SO S√ÅNH:")
                comp = metrics["comparison"]
                print(f"  Accuracy diff:  {comp['accuracy_diff']:+.4f} ({comp['winner_accuracy'].upper()} t·ªët h∆°n)")
                print(f"  Precision diff: {comp['precision_diff']:+.4f}")
                print(f"  Recall diff:    {comp['recall_diff']:+.4f}")
                print(f"  F1-Score diff:  {comp['f1_diff']:+.4f} ({comp['winner_f1'].upper()} t·ªët h∆°n)")
        else:
            # Kh√¥ng c√≥ ground truth - ch·ªâ so s√°nh
            metrics = results["metrics"]
            
            print("\n" + "-" * 80)
            print("SO S√ÅNH TR·ª∞C TI·∫æP 2 M√î H√åNH")
            print("-" * 80)
            
            print(f"\nüìä T·ª∑ l·ªá ƒë·ªìng √Ω: {metrics['agreement_rate']:.4f}")
            
            print("\nüîµ VECTOR (ML) MODEL:")
            print(f"  Mean Score: {metrics['vector_model']['mean_score']:.4f}")
            print(f"  Std Score:  {metrics['vector_model']['std_score']:.4f}")
            print(f"  Distribution: {metrics['vector_model']['decision_distribution']}")
            
            print("\nüü¢ GEMINI (LLM) MODEL:")
            print(f"  Mean Score: {metrics['gemini_model']['mean_score']:.4f}")
            print(f"  Std Score:  {metrics['gemini_model']['std_score']:.4f}")
            print(f"  Distribution: {metrics['gemini_model']['decision_distribution']}")
            
            print(f"\nüìà Score difference: {metrics['comparison']['score_diff']:+.4f}")
        
        # Ph√¢n t√≠ch methods ƒë∆∞·ª£c s·ª≠ d·ª•ng
        print("\n" + "-" * 80)
        print("PH∆Ø∆†NG PH√ÅP ƒê∆Ø·ª¢C S·ª¨ D·ª§NG")
        print("-" * 80)
        
        from collections import Counter
        
        if results.get('enable_vector') and len(results['vector_methods']) > 0:
            vector_methods_count = Counter(results['vector_methods'])
            print("\nüîµ VECTOR MODEL methods:")
            for method, count in vector_methods_count.items():
                print(f"  {method}: {count} ({count/len(results['vector_methods'])*100:.1f}%)")
        
        if results.get('enable_llm') and len(results['gemini_methods']) > 0:
            gemini_methods_count = Counter(results['gemini_methods'])
            print("\nüü¢ GEMINI MODEL methods:")
            for method, count in gemini_methods_count.items():
                print(f"  {method}: {count} ({count/len(results['gemini_methods'])*100:.1f}%)")
    
    def save_results(self, results: Dict, output_path: str = "logs/decision_evaluation.json"):
        """L∆∞u k·∫øt qu·∫£ v√†o file JSON"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Th√™m timestamp
        results_with_meta = {
            "timestamp": datetime.now().isoformat(),
            "evaluation_results": results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_with_meta, f, indent=2, ensure_ascii=False)
        
        print(f"\n  ‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {output_path}")


def main():
    """V√≠ d·ª• s·ª≠ d·ª•ng h·ªá th·ªëng ƒë√°nh gi√°"""
    print("=" * 80)
    print("KH·ªûI T·∫†O H·ªÜ TH·ªêNG ƒê√ÅNH GI√Å")
    print("=" * 80)
    
    # Load ho·∫∑c train h·ªá th·ªëng
    system = MASClinicalDecisionSystem()
    
    csv_path = "data/Bacteria_dataset_Multiresictance.csv"
    if not os.path.exists(csv_path):
        csv_path = "Bacteria_dataset_Multiresictance.csv"
    
    if not os.path.exists(csv_path):
        print("‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu.")
        return
    
    # Ki·ªÉm tra xem ƒë√£ train ch∆∞a
    if not system.is_trained:
        print("  ‚ö†Ô∏è  H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c train. ƒêang train...")
        system.train(csv_path, test_size=0.2, random_state=42)
    else:
        print("  ‚úì H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c train. ƒêang load...")
        try:
            system.load()
        except:
            print("  ‚ö†Ô∏è  Kh√¥ng th·ªÉ load. ƒêang train l·∫°i...")
            system.train(csv_path, test_size=0.2, random_state=42)
    
    # Ch·ªçn m√¥ h√¨nh ƒë·ªÉ ƒë√°nh gi√° (c√≥ th·ªÉ enable c·∫£ 2 ho·∫∑c ch·ªâ 1)
    enable_vector = True   # Set False ƒë·ªÉ t·∫Øt Vector model
    enable_llm = True      # Set False ƒë·ªÉ t·∫Øt Gemini LLM model
    
    # L·∫•y Gemini API key t·ª´ environment variable (t·ª± ƒë·ªông load t·ª´ .env n·∫øu c√≥)
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        print("‚ö†Ô∏è  C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y GEMINI_API_KEY trong environment variables!")
        print("   Vui l√≤ng ƒë·∫∑t API key b·∫±ng m·ªôt trong c√°c c√°ch sau:")
        print("   1. T·∫°o file .env v·ªõi n·ªôi dung: GEMINI_API_KEY=your-api-key-here (KHUY·∫æN NGH·ªä)")
        print("   2. Windows PowerShell: $env:GEMINI_API_KEY='your-api-key-here'")
        print("   3. Windows CMD: set GEMINI_API_KEY=your-api-key-here")
        print("   4. Linux/Mac: export GEMINI_API_KEY='your-api-key-here'")
        print("\n   L·∫•y API key m·ªõi t·∫°i: https://aistudio.google.com/app/apikey")
        print("   ‚ö†Ô∏è  KH√îNG BAO GI·ªú hardcode API key trong code!")
        print("   üìñ Xem h∆∞·ªõng d·∫´n chi ti·∫øt trong file ENV_SETUP.md")
        if enable_llm:
            print("\n   ‚ö†Ô∏è  LLM s·∫Ω kh√¥ng ho·∫°t ƒë·ªông n·∫øu kh√¥ng c√≥ API key h·ª£p l·ªá.")
            response = input("\n   B·∫°n c√≥ mu·ªën ti·∫øp t·ª•c m√† kh√¥ng d√πng LLM? (y/n): ")
            if response.lower() != 'y':
                print("   ƒêang d·ª´ng...")
                return
            enable_llm = False
    
    # T·∫°o evaluator
    evaluator = DecisionModelEvaluator(
        system, 
        gemini_api_key=gemini_api_key,
        enable_vector=enable_vector,
        enable_llm=enable_llm
    )
    
    # ƒê√°nh gi√° tr√™n dataset (c√≥ th·ªÉ gi·ªõi h·∫°n s·ªë m·∫´u ƒë·ªÉ test nhanh)
    print("\n" + "=" * 80)
    print("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å")
    print("=" * 80)
    
    # Th·ªùi gian delay gi·ªØa c√°c l·∫ßn g·ªçi LLM (gi√¢y) ƒë·ªÉ tr√°nh quota
    # ƒê·∫∑t 0 ƒë·ªÉ kh√¥ng delay, ho·∫∑c 15 ƒë·ªÉ delay 15 gi√¢y gi·ªØa m·ªói m·∫´u
    llm_delay = 15.0  # Delay 15 gi√¢y gi·ªØa m·ªói l·∫ßn g·ªçi LLM
    
    results = evaluator.evaluate_on_dataset(
        csv_path,
        n_samples=20,  # Gi·ªõi h·∫°n 50 m·∫´u ƒë·ªÉ test nhanh, c√≥ th·ªÉ tƒÉng ho·∫∑c ƒë·ªÉ None
        use_ground_truth=True,
        llm_delay=llm_delay  # Delay gi·ªØa c√°c l·∫ßn g·ªçi LLM
    )
    
    # In k·∫øt qu·∫£
    evaluator.print_results(results)
    
    # L∆∞u k·∫øt qu·∫£
    evaluator.save_results(results)


if __name__ == "__main__":
    main()

