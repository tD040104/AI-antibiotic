"""
H·ªá th·ªëng ƒë√°nh gi√° v√† so s√°nh 2 m√¥ h√¨nh decision:
1. Vector (ML) - s·ª≠ d·ª•ng decision tree v√† fuzzy logic v·ªõi vector inputs
2. Gemini (LLM) - s·ª≠ d·ª•ng Gemini API ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh
"""

from __future__ import annotations

import os
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report
)
import json
from datetime import datetime

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenv not installed, try manual loading
    try:
        if os.path.exists('.env'):
            with open('.env', 'r', encoding='utf-8-sig') as f:  # utf-8-sig removes BOM
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
    except Exception:
        pass
except Exception:
    # If load_dotenv fails, try manual loading
    try:
        if os.path.exists('.env'):
            with open('.env', 'r', encoding='utf-8-sig') as f:  # utf-8-sig removes BOM
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        os.environ[key.strip()] = value.strip()
    except Exception:
        pass

from main import MASClinicalDecisionSystem
from agents.decision_agent import DecisionAgent, LLMDecisionMaker


class DecisionModelEvaluator:
    """ƒê√°nh gi√° v√† so s√°nh 2 m√¥ h√¨nh decision: Vector (ML) v√† Gemini (LLM)"""
    
    def __init__(
        self,
        system: MASClinicalDecisionSystem,
        gemini_api_key: Optional[str] = None,
        enable_vector: bool = True,
        enable_llm: bool = True
    ):
        """
        Args:
            system: MASClinicalDecisionSystem ƒë√£ ƒë∆∞·ª£c train
            gemini_api_key: API key cho Gemini (n·∫øu d√πng LLM)
            enable_vector: Enable Vector (ML) model
            enable_llm: Enable Gemini (LLM) model
        """
        self.system = system
        self.enable_vector = enable_vector
        self.enable_llm = enable_llm
        
        # T·∫°o decision agents d·ª±a tr√™n flags
        self.vector_agent = None
        self.gemini_agent = None
        
        if enable_vector:
            # Agent 1: Ch·ªâ d√πng Vector (ML) - kh√¥ng d√πng LLM
            self.vector_agent = DecisionAgent(
                use_fuzzy=True,
                use_decision_tree=True,
                use_llm=False,
                decision_mode="vector_only"  # Ch·ªâ d√πng vector
            )
            print(f"  ‚úì Vector (ML) model ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t")
        
        if enable_llm:
            # Agent 2: Ch·ªâ d√πng Gemini (LLM)
            self.gemini_agent = DecisionAgent(
                use_fuzzy=True,
                use_decision_tree=True,
                use_llm=True,
                llm_api_key=gemini_api_key,
                decision_mode="llm_only"  # Ch·ªâ d√πng LLM
            )
        
            # Ki·ªÉm tra xem LLM c√≥ available kh√¥ng
            if self.gemini_agent.llm_decision_maker:
                if self.gemini_agent.llm_decision_maker.available:
                    print(f"  ‚úì Gemini LLM ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t")
                    # Test LLM v·ªõi m·ªôt sample ƒë∆°n gi·∫£n
                    try:
                        test_result = self.gemini_agent.llm_decision_maker.generate_decision(
                            patient_data={"age": 50, "gender": "M"},
                            probabilities={"AMX/AMP": 0.8, "CIP": 0.6},
                            critic_report={"decision": {"uncertainty_score": 0.2}},
                            explanation={"decision": {"decision_score": 0.7}}
                        )
                        if test_result.get("decision_type") not in ["llm_unavailable", "llm_error"]:
                            print(f"  ‚úì Test LLM th√†nh c√¥ng: {test_result.get('decision_type')}")
                        else:
                            print(f"  ‚ö†Ô∏è  Test LLM th·∫•t b·∫°i: {test_result.get('decision_type')}")
                            if test_result.get("error"):
                                print(f"     Error: {test_result.get('error')}")
                    except Exception as e:
                        print(f"  ‚ö†Ô∏è  Test LLM exception: {str(e)}")
                else:
                    print(f"  ‚ö†Ô∏è  Gemini LLM kh√¥ng available (ki·ªÉm tra API key)")
            else:
                print(f"  ‚ö†Ô∏è  LLMDecisionMaker ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        
        if not enable_vector and not enable_llm:
            raise ValueError("Ph·∫£i enable √≠t nh·∫•t 1 m√¥ h√¨nh (enable_vector ho·∫∑c enable_llm)")
    
    def _normalize_decision_type(self, decision_type: str) -> str:
        """Chu·∫©n h√≥a decision_type v·ªÅ 3 lo·∫°i: treat, review, test"""
        decision_lower = decision_type.lower()
        
        if "treat" in decision_lower or "high_confidence" in decision_lower:
            return "treat"
        elif "test" in decision_lower or "additional" in decision_lower or "requires" in decision_lower:
            return "test"
        else:
            return "review"
    
    def _get_ground_truth_decision(
        self,
        probabilities: pd.DataFrame,
        patient_features: Optional[pd.Series],
        critic_report: Dict
    ) -> str:
        """
        T·∫°o ground truth decision d·ª±a tr√™n heuristic:
        - treat: n·∫øu c√≥ √≠t nh·∫•t 1 antibiotic v·ªõi prob >= 0.7 v√† uncertainty th·∫•p
        - test: n·∫øu uncertainty cao ho·∫∑c kh√¥ng c√≥ antibiotic n√†o v·ªõi prob >= 0.5
        - review: c√°c tr∆∞·ªùng h·ª£p c√≤n l·∫°i
        """
        if probabilities.empty:
            return "test"
        
        proba_series = probabilities.iloc[0]
        max_prob = proba_series.max()
        mean_prob = proba_series.mean()
        
        uncertainty = critic_report.get("decision", {}).get("uncertainty_score", 0.5)
        risk_factors = patient_features.get("Total_risk_factors", 0) if patient_features is not None else 0
        
        # Heuristic rules
        if max_prob >= 0.7 and uncertainty < 0.3 and risk_factors < 3:
            return "treat"
        elif max_prob < 0.5 or uncertainty > 0.7 or risk_factors >= 4:
            return "test"
        else:
            return "review"
    
    def evaluate_on_dataset(
        self,
        csv_path: str,
        n_samples: Optional[int] = None,
        use_ground_truth: bool = True
    ) -> Dict:
        """
        ƒê√°nh gi√° 2 m√¥ h√¨nh tr√™n dataset.
        
        Args:
            csv_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV
            n_samples: S·ªë l∆∞·ª£ng m·∫´u ƒë·ªÉ ƒë√°nh gi√° (None = t·∫•t c·∫£)
            use_ground_truth: N·∫øu True, s·ª≠ d·ª•ng ground truth heuristic. N·∫øu False, ch·ªâ so s√°nh 2 m√¥ h√¨nh v·ªõi nhau.
        """
        print("=" * 80)
        print("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å 2 M√î H√åNH DECISION")
        print("=" * 80)
        
        # ƒê·ªçc d·ªØ li·ªáu
        df = pd.read_csv(csv_path)
        if n_samples:
            df = df.head(n_samples)
        
        print(f"  ‚úì ƒê√£ t·∫£i {len(df)} m·∫´u t·ª´ dataset")
        
        # K·∫øt qu·∫£ - ch·ªâ kh·ªüi t·∫°o cho c√°c m√¥ h√¨nh ƒë∆∞·ª£c enable
        vector_predictions = []
        gemini_predictions = []
        ground_truths = []
        vector_scores = []
        gemini_scores = []
        vector_methods = []
        gemini_methods = []
        
        # Ki·ªÉm tra c√≥ √≠t nh·∫•t 1 m√¥ h√¨nh ƒë∆∞·ª£c enable
        if not self.enable_vector and not self.enable_llm:
            raise ValueError("Ph·∫£i enable √≠t nh·∫•t 1 m√¥ h√¨nh ƒë·ªÉ ƒë√°nh gi√°")
        
        print("\n  ƒêang ch·∫°y ƒë√°nh gi√° tr√™n t·ª´ng m·∫´u...")
        
        for idx, row in df.iterrows():
            if (idx + 1) % 10 == 0:
                print(f"    ƒê√£ x·ª≠ l√Ω {idx + 1}/{len(df)} m·∫´u...")
            
            try:
                # Chu·∫©n b·ªã d·ªØ li·ªáu b·ªánh nh√¢n
                patient = {
                    "age/gender": row.get("age/gender", ""),
                    "Souches": row.get("Souches", ""),
                    "Diabetes": "Yes" if row.get("Diabetes") in ["Yes", True, 1] else "No",
                    "Hypertension": "Yes" if row.get("Hypertension") in ["Yes", True, 1] else "No",
                    "Hospital_before": "Yes" if row.get("Hospital_before") in ["Yes", True, 1] else "No",
                    "Infection_Freq": row.get("Infection_Freq", 0.0),
                    "Collection_Date": row.get("Collection_Date", ""),
                }
                
                # Ch·∫°y pipeline ƒë·ªÉ l·∫•y c√°c inputs c·∫ßn thi·∫øt
                result = self.system.predict(patient)
                
                # Chuy·ªÉn ƒë·ªïi dict th√†nh DataFrame/Series ƒë√∫ng c√°ch
                # probabilities v√† predictions t·ª´ result l√† dict, c·∫ßn chuy·ªÉn th√†nh DataFrame
                probabilities_dict = result["probabilities"]
                predictions_dict = result["predictions"]
                
                # T·∫°o DataFrame v·ªõi index [0] ƒë·ªÉ c√≥ 1 row
                # ƒê·∫£m b·∫£o DataFrame c√≥ ƒë√∫ng c·∫•u tr√∫c
                probabilities = pd.DataFrame([probabilities_dict], index=[0])
                predictions = pd.DataFrame([predictions_dict], index=[0])
                patient_series = pd.Series(result["features"])
                
                # Ki·ªÉm tra DataFrame kh√¥ng r·ªóng v√† c√≥ d·ªØ li·ªáu h·ª£p l·ªá
                if probabilities.empty or len(probabilities) == 0:
                    print(f"    ‚ö†Ô∏è  Probabilities r·ªóng cho m·∫´u {idx}")
                    continue
                
                # ƒê·∫£m b·∫£o predictions v√† probabilities c√≥ c√πng s·ªë c·ªôt
                if len(predictions.columns) == 0 or len(probabilities.columns) == 0:
                    print(f"    ‚ö†Ô∏è  D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá cho m·∫´u {idx}")
                    continue
                
                critic_report = result["critic_report"]
                explanation = result["explanation"]
                
                # L·∫•y vectors t·ª´ Agent 3 v√† Agent 4
                # explain_vector v√† review_vector c·∫ßn DataFrame, kh√¥ng ph·∫£i Series
                try:
                    explain_vector = self.system.pipeline.explain_agent.explain_vector(
                        patient_series,
                        predictions,  # DataFrame, kh√¥ng ph·∫£i Series
                        probabilities  # DataFrame, kh√¥ng ph·∫£i Series
                    )
                    critic_vector = self.system.pipeline.critic_agent.review_vector(
                        probabilities,
                        patient_series
                    )
                except Exception as vec_error:
                    print(f"    ‚ö†Ô∏è  L·ªói khi t·∫°o vectors cho m·∫´u {idx}: {str(vec_error)}")
                    import traceback
                    traceback.print_exc()
                    continue
                
                # 1. Ch·∫°y Vector (ML) model (n·∫øu ƒë∆∞·ª£c enable)
                if self.enable_vector and self.vector_agent:
                    vector_decision = self.vector_agent.decide(
                        probabilities,
                        critic_report,
                        patient_series,
                        explanation=explanation,
                        explain_vector=explain_vector,
                        critic_vector=critic_vector
                    )
                    
                    vector_decision_type = self._normalize_decision_type(
                        vector_decision["decision"].get("decision_type", "review")
                    )
                    vector_predictions.append(vector_decision_type)
                    vector_scores.append(vector_decision["decision"].get("decision_score", 0.5))
                    vector_methods.append(vector_decision["decision"].get("method", "unknown"))
                
                # 2. Ch·∫°y Gemini (LLM) model (n·∫øu ƒë∆∞·ª£c enable)
                if self.enable_llm and self.gemini_agent:
                    # Kh√¥ng truy·ªÅn vectors cho LLM-only mode ƒë·ªÉ force s·ª≠ d·ª•ng LLM
                    gemini_decision = self.gemini_agent.decide(
                        probabilities,
                        critic_report,
                        patient_series,
                        explanation=explanation,
                        explain_vector=None,  # Kh√¥ng truy·ªÅn vectors ƒë·ªÉ force LLM
                        critic_vector=None
                    )
                    
                    gemini_decision_type = self._normalize_decision_type(
                        gemini_decision["decision"].get("decision_type", "review")
                    )
                    gemini_predictions.append(gemini_decision_type)
                    gemini_scores.append(gemini_decision["decision"].get("decision_score", 0.5))
                    gemini_method = gemini_decision["decision"].get("method", "unknown")
                    gemini_methods.append(gemini_method)
                    
                    # Debug: Log n·∫øu kh√¥ng d√πng LLM (ch·ªâ log 1 l·∫ßn ƒë·∫ßu ti√™n)
                    if idx == 0 and "llm" not in gemini_method.lower() and "gemini" not in gemini_method.lower():
                        print(f"\n    ‚ö†Ô∏è  DEBUG: Gemini model ƒëang d√πng method '{gemini_method}' thay v√¨ LLM")
                        if gemini_decision["decision"].get("error"):
                            print(f"    ‚ö†Ô∏è  LLM Error: {gemini_decision['decision'].get('error')}")
                
                # 3. Ground truth (n·∫øu c·∫ßn) - ch·ªâ t√≠nh 1 l·∫ßn cho m·ªói m·∫´u
                if use_ground_truth and len(ground_truths) < len(vector_predictions) + len(gemini_predictions):
                    gt = self._get_ground_truth_decision(
                        probabilities,
                        patient_series,
                        critic_report
                    )
                    ground_truths.append(gt)
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è  L·ªói khi x·ª≠ l√Ω m·∫´u {idx}: {str(e)}")
                continue
        
        # T√≠nh s·ªë m·∫´u ƒë√£ x·ª≠ l√Ω
        n_processed = max(len(vector_predictions), len(gemini_predictions))
        print(f"\n  ‚úì ƒê√£ ho√†n th√†nh ƒë√°nh gi√° {n_processed} m·∫´u")
        
        # Debug: Ki·ªÉm tra ph√¢n b·ªë predictions
        if self.enable_llm and len(gemini_predictions) > 0:
            from collections import Counter
            gemini_dist = Counter(gemini_predictions)
            print(f"\n  üìä Ph√¢n b·ªë Gemini predictions: {dict(gemini_dist)}")
            if len(gemini_dist) == 1:
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: T·∫•t c·∫£ Gemini predictions ƒë·ªÅu gi·ªëng nhau: {list(gemini_dist.keys())[0]}")
        
        if use_ground_truth and len(ground_truths) > 0:
            from collections import Counter
            gt_dist = Counter(ground_truths)
            print(f"  üìä Ph√¢n b·ªë Ground Truth: {dict(gt_dist)}")
            if len(gt_dist) == 1:
                print(f"  ‚ö†Ô∏è  C·∫¢NH B√ÅO: T·∫•t c·∫£ Ground Truth ƒë·ªÅu gi·ªëng nhau: {list(gt_dist.keys())[0]}")
        
        # T√≠nh metrics
        results = {
            "n_samples": n_processed,
            "enable_vector": self.enable_vector,
            "enable_llm": self.enable_llm,
            "vector_predictions": vector_predictions if self.enable_vector else [],
            "gemini_predictions": gemini_predictions if self.enable_llm else [],
            "vector_scores": vector_scores if self.enable_vector else [],
            "gemini_scores": gemini_scores if self.enable_llm else [],
            "vector_methods": vector_methods if self.enable_vector else [],
            "gemini_methods": gemini_methods if self.enable_llm else [],
        }
        
        if use_ground_truth:
            results["ground_truths"] = ground_truths
            results["metrics"] = self._calculate_metrics_with_ground_truth(
                ground_truths,
                vector_predictions if self.enable_vector else [],
                gemini_predictions if self.enable_llm else []
            )
        else:
            results["metrics"] = self._calculate_metrics_comparison(
                vector_predictions if self.enable_vector else [],
                gemini_predictions if self.enable_llm else [],
                vector_scores if self.enable_vector else [],
                gemini_scores if self.enable_llm else []
            )
        
        return results
    
    def _calculate_metrics_with_ground_truth(
        self,
        ground_truths: List[str],
        vector_predictions: List[str],
        gemini_predictions: List[str]
    ) -> Dict:
        """T√≠nh metrics khi c√≥ ground truth"""
        labels = ["treat", "review", "test"]
        metrics = {}
        
        # Metrics cho Vector model (n·∫øu ƒë∆∞·ª£c enable)
        if self.enable_vector and len(vector_predictions) > 0:
            vector_accuracy = accuracy_score(ground_truths, vector_predictions)
            vector_precision = precision_score(ground_truths, vector_predictions, labels=labels, average="weighted", zero_division=0)
            vector_recall = recall_score(ground_truths, vector_predictions, labels=labels, average="weighted", zero_division=0)
            vector_f1 = f1_score(ground_truths, vector_predictions, labels=labels, average="weighted", zero_division=0)
            vector_cm = confusion_matrix(ground_truths, vector_predictions, labels=labels)
            vector_report = classification_report(ground_truths, vector_predictions, labels=labels, output_dict=True, zero_division=0)
            
            metrics["vector_model"] = {
                "accuracy": float(vector_accuracy),
                "precision": float(vector_precision),
                "recall": float(vector_recall),
                "f1_score": float(vector_f1),
                "confusion_matrix": vector_cm.tolist(),
                "classification_report": vector_report
            }
        
        # Metrics cho Gemini model (n·∫øu ƒë∆∞·ª£c enable)
        if self.enable_llm and len(gemini_predictions) > 0:
            gemini_accuracy = accuracy_score(ground_truths, gemini_predictions)
            gemini_precision = precision_score(ground_truths, gemini_predictions, labels=labels, average="weighted", zero_division=0)
            gemini_recall = recall_score(ground_truths, gemini_predictions, labels=labels, average="weighted", zero_division=0)
            gemini_f1 = f1_score(ground_truths, gemini_predictions, labels=labels, average="weighted", zero_division=0)
            gemini_cm = confusion_matrix(ground_truths, gemini_predictions, labels=labels)
            gemini_report = classification_report(ground_truths, gemini_predictions, labels=labels, output_dict=True, zero_division=0)
            
            metrics["gemini_model"] = {
                "accuracy": float(gemini_accuracy),
                "precision": float(gemini_precision),
                "recall": float(gemini_recall),
                "f1_score": float(gemini_f1),
                "confusion_matrix": gemini_cm.tolist(),
                "classification_report": gemini_report
            }
        
        # So s√°nh (ch·ªâ khi c·∫£ 2 ƒë·ªÅu ƒë∆∞·ª£c enable)
        if self.enable_vector and self.enable_llm and len(vector_predictions) > 0 and len(gemini_predictions) > 0:
            vector_accuracy = metrics["vector_model"]["accuracy"]
            gemini_accuracy = metrics["gemini_model"]["accuracy"]
            vector_f1 = metrics["vector_model"]["f1_score"]
            gemini_f1 = metrics["gemini_model"]["f1_score"]
            vector_precision = metrics["vector_model"]["precision"]
            gemini_precision = metrics["gemini_model"]["precision"]
            vector_recall = metrics["vector_model"]["recall"]
            gemini_recall = metrics["gemini_model"]["recall"]
            
            metrics["comparison"] = {
                "accuracy_diff": float(gemini_accuracy - vector_accuracy),
                "precision_diff": float(gemini_precision - vector_precision),
                "recall_diff": float(gemini_recall - vector_recall),
                "f1_diff": float(gemini_f1 - vector_f1),
                "winner_accuracy": "gemini" if gemini_accuracy > vector_accuracy else "vector",
                "winner_f1": "gemini" if gemini_f1 > vector_f1 else "vector"
            }
        
        return metrics
    
    def _calculate_metrics_comparison(
        self,
        vector_predictions: List[str],
        gemini_predictions: List[str],
        vector_scores: List[float],
        gemini_scores: List[float]
    ) -> Dict:
        """T√≠nh metrics khi so s√°nh tr·ª±c ti·∫øp 2 m√¥ h√¨nh (kh√¥ng c√≥ ground truth)"""
        # Agreement rate
        agreement = sum(1 for v, g in zip(vector_predictions, gemini_predictions) if v == g)
        agreement_rate = agreement / len(vector_predictions) if vector_predictions else 0
        
        # Score statistics
        vector_mean_score = np.mean(vector_scores) if vector_scores else 0
        gemini_mean_score = np.mean(gemini_scores) if gemini_scores else 0
        vector_std_score = np.std(vector_scores) if vector_scores else 0
        gemini_std_score = np.std(gemini_scores) if gemini_scores else 0
        
        # Distribution of decisions
        from collections import Counter
        vector_dist = Counter(vector_predictions)
        gemini_dist = Counter(gemini_predictions)
        
        return {
            "agreement_rate": float(agreement_rate),
            "vector_model": {
                "mean_score": float(vector_mean_score),
                "std_score": float(vector_std_score),
                "decision_distribution": dict(vector_dist)
            },
            "gemini_model": {
                "mean_score": float(gemini_mean_score),
                "std_score": float(gemini_std_score),
                "decision_distribution": dict(gemini_dist)
            },
            "comparison": {
                "score_diff": float(gemini_mean_score - vector_mean_score),
                "agreement_rate": float(agreement_rate)
            }
        }
    
    def print_results(self, results: Dict):
        """In k·∫øt qu·∫£ ƒë√°nh gi√°"""
        print("\n" + "=" * 80)
        print("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH DECISION")
        print("=" * 80)
        
        print(f"\nüìä S·ªë l∆∞·ª£ng m·∫´u ƒë√°nh gi√°: {results['n_samples']}")
        print(f"üìä M√¥ h√¨nh ƒë∆∞·ª£c ƒë√°nh gi√°:")
        if results.get('enable_vector'):
            print(f"  ‚úì Vector (ML) model")
        if results.get('enable_llm'):
            print(f"  ‚úì Gemini (LLM) model")
        
        if "ground_truths" in results:
            # C√≥ ground truth
            metrics = results["metrics"]
            
            print("\n" + "-" * 80)
            print("METRICS V·ªöI GROUND TRUTH")
            print("-" * 80)
            
            if results.get('enable_vector') and 'vector_model' in metrics:
                print("\nüîµ VECTOR (ML) MODEL:")
                print(f"  Accuracy:  {metrics['vector_model']['accuracy']:.4f}")
                print(f"  Precision: {metrics['vector_model']['precision']:.4f}")
                print(f"  Recall:    {metrics['vector_model']['recall']:.4f}")
                print(f"  F1-Score:  {metrics['vector_model']['f1_score']:.4f}")
                
                print("\nüìä CONFUSION MATRIX - VECTOR MODEL:")
                print("     treat  review  test")
                cm = metrics['vector_model']['confusion_matrix']
                labels = ["treat", "review", "test"]
                for i, label in enumerate(labels):
                    print(f"{label:5} {cm[i]}")
            
            if results.get('enable_llm') and 'gemini_model' in metrics:
                print("\nüü¢ GEMINI (LLM) MODEL:")
                print(f"  Accuracy:  {metrics['gemini_model']['accuracy']:.4f}")
                print(f"  Precision: {metrics['gemini_model']['precision']:.4f}")
                print(f"  Recall:    {metrics['gemini_model']['recall']:.4f}")
                print(f"  F1-Score:  {metrics['gemini_model']['f1_score']:.4f}")
                
                print("\nüìä CONFUSION MATRIX - GEMINI MODEL:")
                print("     treat  review  test")
                cm = metrics['gemini_model']['confusion_matrix']
                labels = ["treat", "review", "test"]
                for i, label in enumerate(labels):
                    print(f"{label:5} {cm[i]}")
            
            if 'comparison' in metrics:
                print("\nüìà SO S√ÅNH:")
                comp = metrics["comparison"]
                print(f"  Accuracy diff:  {comp['accuracy_diff']:+.4f} ({comp['winner_accuracy'].upper()} t·ªët h∆°n)")
                print(f"  Precision diff: {comp['precision_diff']:+.4f}")
                print(f"  Recall diff:    {comp['recall_diff']:+.4f}")
                print(f"  F1-Score diff:  {comp['f1_diff']:+.4f} ({comp['winner_f1'].upper()} t·ªët h∆°n)")
        else:
            # Kh√¥ng c√≥ ground truth - ch·ªâ so s√°nh
            metrics = results["metrics"]
            
            print("\n" + "-" * 80)
            print("SO S√ÅNH TR·ª∞C TI·∫æP 2 M√î H√åNH")
            print("-" * 80)
            
            print(f"\nüìä T·ª∑ l·ªá ƒë·ªìng √Ω: {metrics['agreement_rate']:.4f}")
            
            print("\nüîµ VECTOR (ML) MODEL:")
            print(f"  Mean Score: {metrics['vector_model']['mean_score']:.4f}")
            print(f"  Std Score:  {metrics['vector_model']['std_score']:.4f}")
            print(f"  Distribution: {metrics['vector_model']['decision_distribution']}")
            
            print("\nüü¢ GEMINI (LLM) MODEL:")
            print(f"  Mean Score: {metrics['gemini_model']['mean_score']:.4f}")
            print(f"  Std Score:  {metrics['gemini_model']['std_score']:.4f}")
            print(f"  Distribution: {metrics['gemini_model']['decision_distribution']}")
            
            print(f"\nüìà Score difference: {metrics['comparison']['score_diff']:+.4f}")
        
        # Ph√¢n t√≠ch methods ƒë∆∞·ª£c s·ª≠ d·ª•ng
        print("\n" + "-" * 80)
        print("PH∆Ø∆†NG PH√ÅP ƒê∆Ø·ª¢C S·ª¨ D·ª§NG")
        print("-" * 80)
        
        from collections import Counter
        
        if results.get('enable_vector') and len(results['vector_methods']) > 0:
            vector_methods_count = Counter(results['vector_methods'])
            print("\nüîµ VECTOR MODEL methods:")
            for method, count in vector_methods_count.items():
                print(f"  {method}: {count} ({count/len(results['vector_methods'])*100:.1f}%)")
        
        if results.get('enable_llm') and len(results['gemini_methods']) > 0:
            gemini_methods_count = Counter(results['gemini_methods'])
            print("\nüü¢ GEMINI MODEL methods:")
            for method, count in gemini_methods_count.items():
                print(f"  {method}: {count} ({count/len(results['gemini_methods'])*100:.1f}%)")
    
    def save_results(self, results: Dict, output_path: str = "logs/decision_evaluation.json"):
        """L∆∞u k·∫øt qu·∫£ v√†o file JSON"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Th√™m timestamp
        results_with_meta = {
            "timestamp": datetime.now().isoformat(),
            "evaluation_results": results
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_with_meta, f, indent=2, ensure_ascii=False)
        
        print(f"\n  ‚úì ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {output_path}")


def main():
    """V√≠ d·ª• s·ª≠ d·ª•ng h·ªá th·ªëng ƒë√°nh gi√°"""
    print("=" * 80)
    print("KH·ªûI T·∫†O H·ªÜ TH·ªêNG ƒê√ÅNH GI√Å")
    print("=" * 80)
    
    # Load ho·∫∑c train h·ªá th·ªëng
    system = MASClinicalDecisionSystem()
    
    csv_path = "data/Bacteria_dataset_Multiresictance.csv"
    if not os.path.exists(csv_path):
        csv_path = "Bacteria_dataset_Multiresictance.csv"
    
    if not os.path.exists(csv_path):
        print("‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu.")
        return
    
    # Ki·ªÉm tra xem ƒë√£ train ch∆∞a
    if not system.is_trained:
        print("  ‚ö†Ô∏è  H·ªá th·ªëng ch∆∞a ƒë∆∞·ª£c train. ƒêang train...")
        system.train(csv_path, test_size=0.2, random_state=42)
    else:
        print("  ‚úì H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c train. ƒêang load...")
        try:
            system.load()
        except:
            print("  ‚ö†Ô∏è  Kh√¥ng th·ªÉ load. ƒêang train l·∫°i...")
            system.train(csv_path, test_size=0.2, random_state=42)
    
    # Ch·ªçn m√¥ h√¨nh ƒë·ªÉ ƒë√°nh gi√° (c√≥ th·ªÉ enable c·∫£ 2 ho·∫∑c ch·ªâ 1)
    enable_vector = False   # Set False ƒë·ªÉ t·∫Øt Vector model
    enable_llm = True      # Set False ƒë·ªÉ t·∫Øt Gemini LLM model
    
    # L·∫•y Gemini API key t·ª´ environment variable (t·ª± ƒë·ªông load t·ª´ .env n·∫øu c√≥)
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        print("‚ö†Ô∏è  C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y GEMINI_API_KEY trong environment variables!")
        print("   Vui l√≤ng ƒë·∫∑t API key b·∫±ng m·ªôt trong c√°c c√°ch sau:")
        print("   1. T·∫°o file .env v·ªõi n·ªôi dung: GEMINI_API_KEY=your-api-key-here (KHUY·∫æN NGH·ªä)")
        print("   2. Windows PowerShell: $env:GEMINI_API_KEY='your-api-key-here'")
        print("   3. Windows CMD: set GEMINI_API_KEY=your-api-key-here")
        print("   4. Linux/Mac: export GEMINI_API_KEY='your-api-key-here'")
        print("\n   L·∫•y API key m·ªõi t·∫°i: https://aistudio.google.com/app/apikey")
        print("   ‚ö†Ô∏è  KH√îNG BAO GI·ªú hardcode API key trong code!")
        print("   üìñ Xem h∆∞·ªõng d·∫´n chi ti·∫øt trong file ENV_SETUP.md")
        if enable_llm:
            print("\n   ‚ö†Ô∏è  LLM s·∫Ω kh√¥ng ho·∫°t ƒë·ªông n·∫øu kh√¥ng c√≥ API key h·ª£p l·ªá.")
            response = input("\n   B·∫°n c√≥ mu·ªën ti·∫øp t·ª•c m√† kh√¥ng d√πng LLM? (y/n): ")
            if response.lower() != 'y':
                print("   ƒêang d·ª´ng...")
                return
            enable_llm = False
    
    # T·∫°o evaluator
    evaluator = DecisionModelEvaluator(
        system, 
        gemini_api_key=gemini_api_key,
        enable_vector=enable_vector,
        enable_llm=enable_llm
    )
    
    # ƒê√°nh gi√° tr√™n dataset (c√≥ th·ªÉ gi·ªõi h·∫°n s·ªë m·∫´u ƒë·ªÉ test nhanh)
    print("\n" + "=" * 80)
    print("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å")
    print("=" * 80)
    
    results = evaluator.evaluate_on_dataset(
        csv_path,
        n_samples=50,  # Gi·ªõi h·∫°n 50 m·∫´u ƒë·ªÉ test nhanh, c√≥ th·ªÉ tƒÉng ho·∫∑c ƒë·ªÉ None
        use_ground_truth=True
    )
    
    # In k·∫øt qu·∫£
    evaluator.print_results(results)
    
    # L∆∞u k·∫øt qu·∫£
    evaluator.save_results(results)


if __name__ == "__main__":
    main()

